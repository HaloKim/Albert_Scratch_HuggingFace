{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Albert Scratch Note",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQel1-Mmkobq"
      },
      "source": [
        "# Hugging Face Albert\r\n",
        ">HuggingFace 라이브러리를 사용하여 한글 언어 모델을 처음부터 훈련하고 자신의 모델을 훈련하는 방법입니다. \r\n",
        "\r\n",
        " - Albert Scratch Train\r\n",
        " - Albert는 Sentence Piece tokenizer를 사용합니다.\r\n",
        " - 전처리가 완료된 데이터셋을 불러와 Tokenizer에 적용합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya6s8Ct6igHP"
      },
      "source": [
        "# Load the Essential modules & files Download."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vewsXjkliWyC"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers \\\r\n",
        "&& cd transformers \\\r\n",
        "&& git checkout a3085020ed0d81d4903c50967687192e3101e770  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25PFC9D4iatV"
      },
      "source": [
        "!pip install ./transformers\r\n",
        "!pip install tensorboardX\r\n",
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuGYKUMLiavz"
      },
      "source": [
        "!mkdir kor \\\r\n",
        "&& cd kor \\\r\n",
        "&& wget https://korquad.github.io/dataset/KorQuAD_v1.0_train.json \\\r\n",
        "&& wget https://korquad.github.io/dataset/KorQuAD_v1.0_dev.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Xc1jb8_i3-y"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH1Fsjs1jWw3"
      },
      "source": [
        "## Albert need sentencepiece tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRICo3n-i3kf"
      },
      "source": [
        "import sentencepiece as spm\r\n",
        "spm.SentencePieceTrainer.train(input='/content/drive/MyDrive/Colab_Notebooks/NLP/datasets/wiki_space_tokenizer.txt', \r\n",
        "                              model_prefix='spiece', vocab_size=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Do4pM3Iii3mp"
      },
      "source": [
        "import os\r\n",
        "try:\r\n",
        "  os.mkdir('kor_model')\r\n",
        "except:    # 예외가 발생했을 때 실행됨\r\n",
        "  print('예외가 발생했습니다.')\r\n",
        "os.rename('spiece.model','kor_model/spiece.model')\r\n",
        "os.rename('spiece.vocab','kor_model/spiece.vocab')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9Vr1UHHje7Y"
      },
      "source": [
        "## Load pre-trained tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgCnY6x9i3o1"
      },
      "source": [
        "from transformers import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuDkeCd3i3rT"
      },
      "source": [
        "#Keep in mind, This is a tokenizer for Albert, unlike the previous one, which is a generic one.\r\n",
        "#We'll load it in the form of Albert Tokenizer.\r\n",
        "tokenizer = AlbertTokenizer.from_pretrained(\"/content/kor_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYXEaGBHjCjW"
      },
      "source": [
        "op = tokenizer.encode(\"멤버십 만료일은 2021년 입니다.\")\r\n",
        "tokenizer.decode(op)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0jqg-IvjClu"
      },
      "source": [
        "#Checking vocabulary size\r\n",
        "vocab_size=tokenizer.vocab_size\r\n",
        "vocab_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcCkYbbcjk66"
      },
      "source": [
        "## Expotr Model & Tokenizer model configuration setting for JSON."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GG_mXHbMjCpG"
      },
      "source": [
        "import json\r\n",
        "\r\n",
        "config = {\r\n",
        "    \"architectures\": [\r\n",
        "        \"AlbertModel\"\r\n",
        "    ],\r\n",
        "\t\"attention_probs_dropout_prob\": 0.1,\r\n",
        "\t\"hidden_act\": \"gelu\",\r\n",
        "\t\"hidden_dropout_prob\": 0.1,\r\n",
        "\t\"hidden_size\": 768,\r\n",
        "\t\"initializer_range\": 0.02,\r\n",
        "\t\"intermediate_size\": 3072,\r\n",
        "\t\"layer_norm_eps\": 1e-05,\r\n",
        "\t\"max_position_embeddings\": 512,\r\n",
        "\t\"model_type\": \"albert\",\r\n",
        "\t\"num_attention_heads\": 12,\r\n",
        "\t\"num_hidden_layers\": 6,\r\n",
        "\t\"type_vocab_size\": 1,\r\n",
        "\t\"vocab_size\": vocab_size\r\n",
        "}\r\n",
        "with open(\"/content/kor_model/config.json\", 'w') as fp:\r\n",
        "    json.dump(config, fp)\r\n",
        "\r\n",
        "\r\n",
        "#Configuration for tokenizer.\r\n",
        "#Note: I set do_lower_case: False, and keep_accents:True\r\n",
        "\r\n",
        "tokenizer_config = {\r\n",
        "\t\"max_len\": 512,\r\n",
        "\t\"model_type\": \"albert\",\r\n",
        "\t\"do_lower_case\":False, \r\n",
        "\t\"keep_accents\":True\r\n",
        "}\r\n",
        "with open(\"/content/kor_model/tokenizer_config.json\", 'w') as fp:\r\n",
        "    json.dump(tokenizer_config, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIvBw7xNjPxV"
      },
      "source": [
        "torch.cuda.empty_cache()\r\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KlCuvT0io4p"
      },
      "source": [
        "# Train Step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl6dAZEziayE"
      },
      "source": [
        "!export SQUAD_DIR=/content/kor \\\r\n",
        "&& python transformers/examples/run_squad.py \\\r\n",
        "  --model_type albert \\\r\n",
        "  --model_name_or_path /content/drive/MyDrive/albert_model \\\r\n",
        "  --output_dir /content/drive/MyDrive/Colab_Notebooks/NLP/ALBERT/QnA \\\r\n",
        "  --config_name /content/drive/MyDrive/albert_model \\\r\n",
        "  --tokenizer_name /content/drive/MyDrive/albert_model \\\r\n",
        "  --do_train \\\r\n",
        "  --do_eval \\\r\n",
        "  --train_file $SQUAD_DIR/KorQuAD_v1.0_train.json \\\r\n",
        "  --predict_file $SQUAD_DIR/KorQuAD_v1.0_dev.json \\\r\n",
        "  --learning_rate 3e-5 \\\r\n",
        "  --num_train_epochs 0.1 \\\r\n",
        "  --max_seq_length 512 \\\r\n",
        "  --doc_stride 128 \\\r\n",
        "#  --overwrite_output_dir \\\r\n",
        "#  --save_steps 1000 \\\r\n",
        "#  --max_answer_length 30\r\n",
        "#  --per_gpu_train_batch_size 12 \\\r\n",
        "#  --threads 4 \\\r\n",
        "#  --version_2_with_negative "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K05-6hjyia0R"
      },
      "source": [
        "torch.cuda.empty_cache()\r\n",
        "gc.collect() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1DOAohWkPVK"
      },
      "source": [
        "# When you load the pre-trained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKic6BROkcyF"
      },
      "source": [
        "## Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrhKR6ZLia2V"
      },
      "source": [
        "from transformers import AlbertTokenizer, AlbertModel\r\n",
        "atokenizer = AlbertTokenizer.from_pretrained(\"/content/drive/MyDrive/kor_model\")\r\n",
        "atokenizer.save_pretrained(\"/content/drive/MyDrive/my_albert\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbYEYQ4yia4a"
      },
      "source": [
        "op = atokenizer.encode(\"멤버십 만료일은 2021년 입니다. 다음 시간에 이용해주세요.\")\r\n",
        "print(atokenizer.decode(op))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3hPnS9ria6z"
      },
      "source": [
        "#I am using chackoint because os not much training\r\n",
        "model = AlbertModel.from_pretrained(\"/content/drive/MyDrive/albert_model/checkpoint-14000\")\r\n",
        "model.save_pretrained(\"/content/drive/MyDrive/my_albert\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiEBVW5AkeMr"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUZbSCKNia9L"
      },
      "source": [
        "tokenizer = AlbertTokenizer.from_pretrained(\"/content/drive/MyDrive/my_albert\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJGp7-n-kgMO"
      },
      "source": [
        "txt = \"멤버십 만료일은 2021년 입니다.\"\r\n",
        "op = tokenizer.encode(txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh5hvzmLkgOr"
      },
      "source": [
        "op\r\n",
        "#See howw it's tokenized!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAYAjP5HkiLt"
      },
      "source": [
        "tokenizer.decode(op[:5]), tokenizer.decode(op[5:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrzFunyikiSv"
      },
      "source": [
        "ps = model(torch.tensor(op).unsqueeze(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDZPevBHkiVN"
      },
      "source": [
        "print(ps[0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}